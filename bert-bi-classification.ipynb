{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-03-21T05:29:15.649448Z",
     "iopub.status.busy": "2024-03-21T05:29:15.648710Z",
     "iopub.status.idle": "2024-03-21T05:29:22.953821Z",
     "shell.execute_reply": "2024-03-21T05:29:22.952921Z",
     "shell.execute_reply.started": "2024-03-21T05:29:15.649415Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# 用于加载bert模型的分词器\n",
    "from transformers import AutoTokenizer\n",
    "# 用于加载bert模型\n",
    "from transformers import AutoModel\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-21T05:29:22.956514Z",
     "iopub.status.busy": "2024-03-21T05:29:22.955945Z",
     "iopub.status.idle": "2024-03-21T05:29:23.000214Z",
     "shell.execute_reply": "2024-03-21T05:29:22.999021Z",
     "shell.execute_reply.started": "2024-03-21T05:29:22.956480Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "# 文本的最大长度\n",
    "text_max_length = 128\n",
    "# 总训练的epochs数，我只是随便定义了个数\n",
    "epochs = 100\n",
    "# 取多少训练集的数据作为验证集\n",
    "validation_ratio = 0.1\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 每多少步，打印一次loss\n",
    "log_per_step = 50\n",
    "\n",
    "# 数据集所在位置\n",
    "dataset_dir = Path(\"/kaggle/input/nlp-getting-started/\")\n",
    "os.makedirs(dataset_dir) if not os.path.exists(dataset_dir) else ''\n",
    "\n",
    "# 模型存储路径\n",
    "model_dir = Path(\"/kaggle/working/\")\n",
    "# 如果模型目录不存在，则创建一个\n",
    "os.makedirs(model_dir) if not os.path.exists(model_dir) else ''\n",
    "\n",
    "print(\"Device:\", device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理\n",
    "加载数据集，查看文本最大长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-21T05:29:23.001887Z",
     "iopub.status.busy": "2024-03-21T05:29:23.001577Z",
     "iopub.status.idle": "2024-03-21T05:29:23.074244Z",
     "shell.execute_reply": "2024-03-21T05:29:23.073077Z",
     "shell.execute_reply.started": "2024-03-21T05:29:23.001861Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
       "1                Forest fire near La Ronge Sask. Canada       1  \n",
       "2     All residents asked to 'shelter in place' are ...       1  \n",
       "3     13,000 people receive #wildfires evacuation or...       1  \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  Two giant cranes holding a bridge collapse int...       1  \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
       "7611  Police investigating after an e-bike collided ...       1  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_data = pandas.read_csv(dataset_dir / 'train.csv')\n",
    "pd_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-21T05:29:23.075858Z",
     "iopub.status.busy": "2024-03-21T05:29:23.075518Z",
     "iopub.status.idle": "2024-03-21T05:29:23.116147Z",
     "shell.execute_reply": "2024-03-21T05:29:23.115092Z",
     "shell.execute_reply.started": "2024-03-21T05:29:23.075831Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target\n",
       "0     Our Deeds are the Reason of this #earthquake M...       1\n",
       "1                Forest fire near La Ronge Sask. Canada       1\n",
       "2     All residents asked to 'shelter in place' are ...       1\n",
       "3     13,000 people receive #wildfires evacuation or...       1\n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1\n",
       "...                                                 ...     ...\n",
       "7608  Two giant cranes holding a bridge collapse int...       1\n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1\n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1\n",
       "7611  Police investigating after an e-bike collided ...       1\n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1\n",
       "\n",
       "[7613 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_data = pandas.read_csv(dataset_dir / 'train.csv')[['text', 'target']]\n",
    "pd_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在使用BERT进行文本分类时，文本序列会被分段成较小的片段，每个片段的长度不能超过BERT模型的最大输入长度。BERT-base模型的最大输入长度为512个token。但是，实际上，通常不会使用整个512个token的长度，因为这会导致模型的计算和内存消耗过高，尤其是在GPU内存有限的情况下。\n",
    "\n",
    "因此，为了在保持模型性能的同时有效利用计算资源，常见的做法是将文本序列截断或填充到一个较小的长度，通常是128或者256。在这个设置下，大多数文本序列都可以被完整地处理，而且不会导致过多的计算资源消耗。\n",
    "\n",
    "选择128作为文本最大长度的原因可能是出于以下考虑：\n",
    "\n",
    "大多数文本序列可以在128个token的长度内完整表示，因此不会丢失太多信息。\n",
    "128是一个相对合理的长度，可以平衡模型性能和计算资源的消耗。\n",
    "在实际应用中，较长的文本序列很少出现，因此选择128不会对大多数样本产生太大影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-21T05:29:23.119894Z",
     "iopub.status.busy": "2024-03-21T05:29:23.119527Z",
     "iopub.status.idle": "2024-03-21T05:29:23.145495Z",
     "shell.execute_reply": "2024-03-21T05:29:23.144473Z",
     "shell.execute_reply.started": "2024-03-21T05:29:23.119868Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7607</th>\n",
       "      <td>#stormchase Violent Record Breaking EF-5 El Re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6852 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target\n",
       "0     Our Deeds are the Reason of this #earthquake M...       1\n",
       "1                Forest fire near La Ronge Sask. Canada       1\n",
       "2     All residents asked to 'shelter in place' are ...       1\n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1\n",
       "5     #RockyFire Update => California Hwy. 20 closed...       1\n",
       "...                                                 ...     ...\n",
       "7607  #stormchase Violent Record Breaking EF-5 El Re...       1\n",
       "7608  Two giant cranes holding a bridge collapse int...       1\n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1\n",
       "7611  Police investigating after an e-bike collided ...       1\n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1\n",
       "\n",
       "[6852 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = pd_data['text'].str.len().max()\n",
    "print(max_length)\n",
    "# 按ratio随机划分训练集和验证集\n",
    "pd_validation_data = pd_data.sample(frac = validation_ratio)\n",
    "pd_train_data = pd_data[~pd_data.index.isin(pd_validation_data.index)]\n",
    "pd_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-21T05:29:23.146868Z",
     "iopub.status.busy": "2024-03-21T05:29:23.146604Z",
     "iopub.status.idle": "2024-03-21T05:29:23.159908Z",
     "shell.execute_reply": "2024-03-21T05:29:23.158905Z",
     "shell.execute_reply.started": "2024-03-21T05:29:23.146844Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Our Deeds are the Reason of this earthquake May ALLAH Forgive us all', 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#定义数据类\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self,mode = 'train'):\n",
    "        super(MyDataset,self).__init__()\n",
    "        self.mode = mode\n",
    "        if mode == 'train':\n",
    "            self.dataset = pd_train_data\n",
    "        elif mode == 'validation':\n",
    "            self.dataset = pd_validation_data\n",
    "        elif mode == 'test':\n",
    "            # 如果是测试模式，则返回推文和id。拿id做target主要是方便后面写入结果。\n",
    "            self.dataset = pandas.read_csv(dataset_dir / 'test.csv')[['text', 'id']]\n",
    "        else:\n",
    "            raise Exception(\"Unknown mode {}\".format(mode))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # 取第index条\n",
    "        data = self.dataset.iloc[index]\n",
    "        # 取其推文，做个简单的数据清理\n",
    "        source = data['text'].replace(\"#\", \"\").replace(\"@\", \"\")\n",
    "        # 取对应的推文\n",
    "        if self.mode == 'test':\n",
    "            # 如果是test，将id做为target\n",
    "            target = data['id']\n",
    "        else:\n",
    "            target = data['target']\n",
    "        # 返回推文和target\n",
    "        return source, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "train_dataset = MyDataset('train')\n",
    "validation_dataset = MyDataset('validation')\n",
    "train_dataset.__getitem__(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-21T05:29:23.161361Z",
     "iopub.status.busy": "2024-03-21T05:29:23.161088Z",
     "iopub.status.idle": "2024-03-21T05:29:23.939446Z",
     "shell.execute_reply": "2024-03-21T05:29:23.938351Z",
     "shell.execute_reply.started": "2024-03-21T05:29:23.161338Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e698d96a48409687b5048698200320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea45be850b6c4f5897d271701a625d29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0ec9cf2b3ac4625a15e4040447e062e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1400b17864b54b668751d7af29a9fac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1045, 1005, 1049, 4083, 2784, 4083,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#使用分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer(\"I'm learning deep learning\", return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个collate_fn函数用于对一个batch的文本数据进行处理，将文本句子转换为tensor，并组成一个batch。下面是函数的具体功能和输入输出：\n",
    "\n",
    "输入参数 batch：一个batch的句子，每个句子是一个元组，包含文本和目标标签，例如：[('推文1', 目标1), ('推文2', 目标2), ...]\n",
    "\n",
    "输出：处理后的结果，包含两部分：\n",
    "\n",
    "src：是要送给BERT模型的输入，包含两个tensor：\n",
    "input_ids：经过分词和映射后的输入文本的token id序列。\n",
    "attention_mask：用于指示BERT模型在进行自注意力机制时哪些部分是padding的，需要被忽略。1表示有效token，0表示padding。\n",
    "target：目标标签的tensor序列，即对应每个文本的标签。\n",
    "这个函数首先将输入的batch分成两个列表，一个是文本列表 text，一个是目标标签列表 target。然后使用 tokenizer 对文本进行分词、映射、padding和裁剪等预处理操作，得到模型的输入 src。最后将处理后的输入 src 和目标标签 target 组合成输出。\n",
    "\n",
    "\n",
    "collate_fn函数在数据加载器每次取出一个batch的样本时被调用，用于对这个batch的样本进行预处理、转换成模型所需的格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-21T05:29:23.940949Z",
     "iopub.status.busy": "2024-03-21T05:29:23.940639Z",
     "iopub.status.idle": "2024-03-21T05:29:23.980885Z",
     "shell.execute_reply": "2024-03-21T05:29:23.979991Z",
     "shell.execute_reply.started": "2024-03-21T05:29:23.940922Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: {'input_ids': tensor([[  101, 10482,  6591,  ...,     0,     0,     0],\n",
      "        [  101,  4911,  2474,  ...,     0,     0,     0],\n",
      "        [  101,  5916,  6340,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101, 21318,  2571,  ...,     0,     0,     0],\n",
      "        [  101, 20010, 21149,  ...,     0,     0,     0],\n",
      "        [  101, 26934,  5315,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "torch.Size([16, 128])\n",
      "targets: tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    将一个batch的文本句子转成tensor，并组成batch。\n",
    "    :param batch: 一个batch的句子，例如: [('推文', target), ('推文', target), ...]\n",
    "    :return: 处理后的结果，例如：\n",
    "             src: {'input_ids': tensor([[ 101, ..., 102, 0, 0, ...], ...]), 'attention_mask': tensor([[1, ..., 1, 0, ...], ...])}\n",
    "             target：[1, 1, 0, ...]\n",
    "    \"\"\"\n",
    "    text, target = zip(*batch)\n",
    "    text, target = list(text), list(target)\n",
    "\n",
    "    # src是要送给bert的，所以不需要特殊处理，直接用tokenizer的结果即可\n",
    "    # padding='max_length' 不够长度的进行填充\n",
    "    # truncation=True 长度过长的进行裁剪\n",
    "    src = tokenizer(text, padding='max_length', max_length=text_max_length, return_tensors='pt', truncation=True)\n",
    "\n",
    "    return src, torch.LongTensor(target)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "inputs, targets = next(iter(train_loader))\n",
    "print(\"inputs:\", inputs)\n",
    "print(inputs['input_ids'].shape)\n",
    "print(\"targets:\", targets)\n",
    "#batch_size = 16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "768是BERT模型中隐藏层的维度大小。BERT模型使用了12层或者24层的Transformer编码器，每一层的隐藏层输出的维度大小为768"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-21T05:19:38.782527Z",
     "iopub.status.busy": "2024-03-21T05:19:38.781873Z",
     "iopub.status.idle": "2024-03-21T05:19:38.791954Z",
     "shell.execute_reply": "2024-03-21T05:19:38.790088Z",
     "shell.execute_reply.started": "2024-03-21T05:19:38.782487Z"
    }
   },
   "source": [
    "nn.Linear(768, 256)：将输入的维度从768降到256，这是一个线性变换（全连接层），将BERT模型输出的768维隐藏表示转换为更低维度的表示。\n",
    "\n",
    "nn.ReLU()：ReLU激活函数，用于引入非线性。在降维后的表示上应用ReLU激活函数，以增加模型的非线性能力。\n",
    "\n",
    "nn.Linear(256, 1)：将256维的表示进一步映射到一个单一的值，用于二分类问题中的概率预测。\n",
    "\n",
    "nn.Sigmoid()：Sigmoid激活函数，将输出值压缩到0到1之间，表示概率值。\n",
    "\n",
    "因此，整个self.predictor模块的作用是将BERT模型的输出映射到一个单一的概率值，用于二分类问题的预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-21T05:29:57.807462Z",
     "iopub.status.busy": "2024-03-21T05:29:57.807026Z",
     "iopub.status.idle": "2024-03-21T05:29:57.818164Z",
     "shell.execute_reply": "2024-03-21T05:29:57.817148Z",
     "shell.execute_reply.started": "2024-03-21T05:29:57.807428Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nlast_hidden_state 的形状是 (batch_size, sequence_length, hidden_size)，其中：\\n\\nbatch_size 是当前批次中样本的数量。\\nsequence_length 是输入序列的长度。\\nhidden_size 是隐藏状态的维度，通常等于BERT模型的隐藏层大小，例如在BERT-base中是768。\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#构建模型\n",
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "\n",
    "        # 加载bert模型\n",
    "        self.bert = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "        # 最后的预测层\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(768, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "        :param src: 分词后的推文数据\n",
    "        \"\"\"\n",
    "\n",
    "        # 将src直接序列解包传入bert，因为bert和tokenizer是一套的，所以可以这么做。\n",
    "        # 得到encoder的输出，用最前面[CLS]的输出作为最终线性层的输入\n",
    "        outputs = self.bert(**src).last_hidden_state[:, 0, :]\n",
    "\n",
    "        # 使用线性层来做最终的预测\n",
    "        return self.predictor(outputs)\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "last_hidden_state 的形状是 (batch_size, sequence_length, hidden_size)，其中：\n",
    "\n",
    "batch_size 是当前批次中样本的数量。\n",
    "sequence_length 是输入序列的长度。\n",
    "hidden_size 是隐藏状态的维度，通常等于BERT模型的隐藏层大小，例如在BERT-base中是768。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-21T05:30:00.009229Z",
     "iopub.status.busy": "2024-03-21T05:30:00.008784Z",
     "iopub.status.idle": "2024-03-21T05:30:05.410718Z",
     "shell.execute_reply": "2024-03-21T05:30:05.409804Z",
     "shell.execute_reply.started": "2024-03-21T05:30:00.009195Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34368a7324d2482cb6c224ed539b4d7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = TextClassificationModel()\n",
    "model = model.to(device)\n",
    "model(inputs.to(device))\n",
    "criteria = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)\n",
    "# 由于inputs是字典类型的，定义一个辅助函数帮助to(device)\n",
    "def to_device(dict_tensors):\n",
    "    result_tensors = {}\n",
    "    for key, value in dict_tensors.items():\n",
    "        result_tensors[key] = value.to(device)\n",
    "    return result_tensors\n",
    "\"\"\"\n",
    "将字典中的张量转移到指定的设备（如GPU）。它接受一个字典，其中键是张量的名称，值是张量本身。\n",
    "然后，它迭代字典中的每个键值对，并将值转移到设备上，最后返回一个具有相同键但值位于指定设备上的新字典\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def validate():\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    total_correct = 0\n",
    "    for inputs, targets in validation_loader:\n",
    "        inputs, targets = to_device(inputs), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criteria(outputs.view(-1), targets.float())\n",
    "        total_loss += float(loss)\n",
    "\n",
    "        correct_num = (((outputs >= 0.5).float() * 1).flatten() == targets).sum()\n",
    "        total_correct += correct_num\n",
    "\n",
    "    return total_correct / len(validation_dataset), total_loss / len(validation_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-21T05:32:40.646197Z",
     "iopub.status.busy": "2024-03-21T05:32:40.645525Z",
     "iopub.status.idle": "2024-03-21T05:49:21.459819Z",
     "shell.execute_reply": "2024-03-21T05:49:21.458236Z",
     "shell.execute_reply.started": "2024-03-21T05:32:40.646159Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Step: 49/429, total loss:27.0852\n",
      "Epoch 1/100, Step: 99/429, total loss:21.9039\n",
      "Epoch 1/100, Step: 149/429, total loss:22.6578\n",
      "Epoch 1/100, Step: 199/429, total loss:21.1815\n",
      "Epoch 1/100, Step: 249/429, total loss:20.3617\n",
      "Epoch 1/100, Step: 299/429, total loss:18.9497\n",
      "Epoch 1/100, Step: 349/429, total loss:20.8270\n",
      "Epoch 1/100, Step: 399/429, total loss:20.0272\n",
      "Epoch 1, accuracy: 0.8279, validation loss: 0.0247\n",
      "Epoch 2/100, Step: 20/429, total loss:18.0542\n",
      "Epoch 2/100, Step: 70/429, total loss:14.7096\n",
      "Epoch 2/100, Step: 120/429, total loss:15.0193\n",
      "Epoch 2/100, Step: 170/429, total loss:14.2937\n",
      "Epoch 2/100, Step: 220/429, total loss:14.1752\n",
      "Epoch 2/100, Step: 270/429, total loss:14.2685\n",
      "Epoch 2/100, Step: 320/429, total loss:14.0682\n",
      "Epoch 2/100, Step: 370/429, total loss:16.1425\n",
      "Epoch 2/100, Step: 420/429, total loss:17.1818\n",
      "Epoch 2, accuracy: 0.8397, validation loss: 0.0279\n",
      "Epoch 3/100, Step: 41/429, total loss:8.0204\n",
      "Epoch 3/100, Step: 91/429, total loss:9.5614\n",
      "Epoch 3/100, Step: 141/429, total loss:9.2036\n",
      "Epoch 3/100, Step: 191/429, total loss:8.9964\n",
      "Epoch 3/100, Step: 241/429, total loss:10.7305\n",
      "Epoch 3/100, Step: 291/429, total loss:10.5000\n",
      "Epoch 3/100, Step: 341/429, total loss:11.3632\n",
      "Epoch 3/100, Step: 391/429, total loss:10.3103\n",
      "Epoch 3, accuracy: 0.8252, validation loss: 0.0339\n",
      "Epoch 4/100, Step: 12/429, total loss:8.1302\n",
      "Epoch 4/100, Step: 62/429, total loss:5.9590\n",
      "Epoch 4/100, Step: 112/429, total loss:6.9333\n",
      "Epoch 4/100, Step: 162/429, total loss:6.4659\n",
      "Epoch 4/100, Step: 212/429, total loss:6.3636\n",
      "Epoch 4/100, Step: 262/429, total loss:6.6609\n",
      "Epoch 4/100, Step: 312/429, total loss:6.3064\n",
      "Epoch 4/100, Step: 362/429, total loss:5.7218\n",
      "Epoch 4/100, Step: 412/429, total loss:6.8676\n",
      "Epoch 4, accuracy: 0.8042, validation loss: 0.0370\n",
      "Epoch 5/100, Step: 33/429, total loss:4.4049\n",
      "Epoch 5/100, Step: 83/429, total loss:3.0673\n",
      "Epoch 5/100, Step: 133/429, total loss:4.1351\n",
      "Epoch 5/100, Step: 183/429, total loss:3.8803\n",
      "Epoch 5/100, Step: 233/429, total loss:3.2633\n",
      "Epoch 5/100, Step: 283/429, total loss:4.6513\n",
      "Epoch 5/100, Step: 333/429, total loss:4.3888\n",
      "Epoch 5/100, Step: 383/429, total loss:5.1710\n",
      "Epoch 5, accuracy: 0.8055, validation loss: 0.0484\n",
      "Epoch 6/100, Step: 4/429, total loss:4.7682\n",
      "Epoch 6/100, Step: 54/429, total loss:3.6112\n",
      "Epoch 6/100, Step: 104/429, total loss:4.2054\n",
      "Epoch 6/100, Step: 154/429, total loss:3.0118\n",
      "Epoch 6/100, Step: 204/429, total loss:3.7317\n",
      "Epoch 6/100, Step: 254/429, total loss:4.3987\n",
      "Epoch 6/100, Step: 304/429, total loss:4.3260\n",
      "Epoch 6/100, Step: 354/429, total loss:4.8860\n",
      "Epoch 6/100, Step: 404/429, total loss:4.0680\n",
      "Epoch 6, accuracy: 0.8200, validation loss: 0.0394\n",
      "Epoch 7/100, Step: 25/429, total loss:4.3479\n",
      "Epoch 7/100, Step: 75/429, total loss:3.1758\n",
      "Epoch 7/100, Step: 125/429, total loss:3.0595\n",
      "Epoch 7/100, Step: 175/429, total loss:3.2737\n",
      "Epoch 7/100, Step: 225/429, total loss:3.4793\n",
      "Epoch 7/100, Step: 275/429, total loss:2.8818\n",
      "Epoch 7/100, Step: 325/429, total loss:4.4013\n",
      "Epoch 7/100, Step: 375/429, total loss:4.0712\n",
      "Epoch 7/100, Step: 425/429, total loss:3.6233\n",
      "Epoch 7, accuracy: 0.8134, validation loss: 0.0505\n",
      "Epoch 8/100, Step: 46/429, total loss:2.4538\n",
      "Epoch 8/100, Step: 96/429, total loss:2.9408\n",
      "Epoch 8/100, Step: 146/429, total loss:2.0388\n",
      "Epoch 8/100, Step: 196/429, total loss:2.2719\n",
      "Epoch 8/100, Step: 246/429, total loss:3.0254\n",
      "Epoch 8/100, Step: 296/429, total loss:3.1964\n",
      "Epoch 8/100, Step: 346/429, total loss:4.5933\n",
      "Epoch 8/100, Step: 396/429, total loss:2.3120\n",
      "Epoch 8, accuracy: 0.8121, validation loss: 0.0532\n",
      "Epoch 9/100, Step: 17/429, total loss:2.1302\n",
      "Epoch 9/100, Step: 67/429, total loss:2.8609\n",
      "Epoch 9/100, Step: 117/429, total loss:2.5762\n",
      "Epoch 9/100, Step: 167/429, total loss:2.5297\n",
      "Epoch 9/100, Step: 217/429, total loss:3.0031\n",
      "Epoch 9/100, Step: 267/429, total loss:2.8550\n",
      "Epoch 9/100, Step: 317/429, total loss:2.7038\n",
      "Epoch 9/100, Step: 367/429, total loss:2.0410\n",
      "Epoch 9/100, Step: 417/429, total loss:1.6495\n",
      "Epoch 9, accuracy: 0.8147, validation loss: 0.0584\n",
      "Epoch 10/100, Step: 38/429, total loss:2.4684\n",
      "Epoch 10/100, Step: 88/429, total loss:2.5976\n",
      "Epoch 10/100, Step: 138/429, total loss:2.2212\n",
      "Epoch 10/100, Step: 188/429, total loss:2.0417\n",
      "Epoch 10/100, Step: 238/429, total loss:1.8892\n",
      "Epoch 10/100, Step: 288/429, total loss:2.2668\n",
      "Epoch 10/100, Step: 338/429, total loss:2.4390\n",
      "Epoch 10/100, Step: 388/429, total loss:2.4793\n",
      "Epoch 10, accuracy: 0.8068, validation loss: 0.0606\n",
      "Epoch 11/100, Step: 9/429, total loss:2.3416\n",
      "Epoch 11/100, Step: 59/429, total loss:2.9766\n",
      "Epoch 11/100, Step: 109/429, total loss:2.4573\n",
      "Epoch 11/100, Step: 159/429, total loss:2.1148\n",
      "Epoch 11/100, Step: 209/429, total loss:1.9889\n",
      "Epoch 11/100, Step: 259/429, total loss:2.2313\n",
      "Epoch 11/100, Step: 309/429, total loss:1.9831\n",
      "Epoch 11/100, Step: 359/429, total loss:1.7014\n",
      "Epoch 11/100, Step: 409/429, total loss:2.2243\n",
      "Epoch 11, accuracy: 0.8173, validation loss: 0.0610\n",
      "Epoch 12/100, Step: 30/429, total loss:1.2329\n",
      "Epoch 12/100, Step: 80/429, total loss:2.0102\n",
      "Epoch 12/100, Step: 130/429, total loss:1.2435\n",
      "Epoch 12/100, Step: 180/429, total loss:1.9317\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 30\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m log_per_step \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 首先将模型调成训练模式\n",
    "model.train()\n",
    "\n",
    "# 清空一下cuda缓存\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# 定义几个变量，帮助打印loss\n",
    "total_loss = 0.\n",
    "# 记录步数\n",
    "step = 0\n",
    "\n",
    "# 记录在验证集上最好的准确率\n",
    "best_accuracy = 0\n",
    "\n",
    "# 开始训练\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        # 从batch中拿到训练数据\n",
    "        inputs, targets = to_device(inputs), targets.to(device)\n",
    "        # 传入模型进行前向传递\n",
    "        outputs = model(inputs)\n",
    "        # 计算损失\n",
    "        loss = criteria(outputs.view(-1), targets.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += float(loss)\n",
    "        step += 1\n",
    "\n",
    "        if step % log_per_step == 0:\n",
    "            print(\"Epoch {}/{}, Step: {}/{}, total loss:{:.4f}\".format(epoch+1, epochs, i, len(train_loader), total_loss))\n",
    "            total_loss = 0\n",
    "\n",
    "        del inputs, targets\n",
    "\n",
    "    # 一个epoch后，使用过验证集进行验证\n",
    "    accuracy, validation_loss = validate()\n",
    "    print(\"Epoch {}, accuracy: {:.4f}, validation loss: {:.4f}\".format(epoch+1, accuracy, validation_loss))\n",
    "    torch.save(model, model_dir / f\"model_{epoch}.pt\")\n",
    "\n",
    "    # 保存最好的模型\n",
    "    if accuracy > best_accuracy:\n",
    "        torch.save(model, model_dir / f\"model_best.pt\")\n",
    "        best_accuracy = accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-21T05:50:41.963198Z",
     "iopub.status.busy": "2024-03-21T05:50:41.962675Z",
     "iopub.status.idle": "2024-03-21T05:50:55.351286Z",
     "shell.execute_reply": "2024-03-21T05:50:55.350207Z",
     "shell.execute_reply.started": "2024-03-21T05:50:41.963154Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3a9abe63a6c4f1ebe78a9421f28610c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(model_dir / f\"model_best.pt\")\n",
    "model = model.eval()\n",
    "test_dataset = MyDataset('test')\n",
    "#构造测试集的dataloader。测试集是不包含target的\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "results = []\n",
    "for inputs, ids in tqdm(test_loader):\n",
    "    outputs = model(inputs.to(device))\n",
    "    outputs = (outputs >= 0.5).int().flatten().tolist()\n",
    "    ids = ids.tolist()\n",
    "    results = results + [(id, result) for result, id in zip(outputs, ids)]\n",
    "with open('/kaggle/working/results.csv', 'w', encoding='utf-8') as f:\n",
    "    f.write('id,target\\n')\n",
    "    for id, result in results:\n",
    "        f.write(f\"{id},{result}\\n\")\n",
    "print(\"Finished!\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 869809,
     "sourceId": 17777,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
